{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987 14987\n",
      "14987\n",
      "14987 14987\n"
     ]
    }
   ],
   "source": [
    "#dummy_data for test\n",
    "def write_file(a,b):\n",
    "    with open('new_result','a') as f:\n",
    "        f.write(str((a,b))+'\\n')\n",
    "batch=10\n",
    "max_sentence = 115\n",
    "max_word = 35\n",
    "\n",
    "with open('word_embed.pkl','rb') as f:\n",
    "    word_embeda= pk.load(f)\n",
    "    \n",
    "    \n",
    "import pickle as pk\n",
    "\n",
    "with open('encoded_sentence_words.pkl','rb') as f:\n",
    "    encoded_sentence_word = pk.load(f)\n",
    "    \n",
    "with open('encoded_sentence_todyy.pkl','rb') as f:\n",
    "    encoded_sentence_char = pk.load(f)\n",
    "    \n",
    "with open('encoded_labels_data.pkl','rb') as f:\n",
    "    encoded_labels_data = pk.load(f)\n",
    "\n",
    "padded_sentence_ , sequence_lengths = encoded_sentence_word['padded_sentence'] , encoded_sentence_word['sequence_len']\n",
    "print(len(padded_sentence_),len(sequence_lengths))\n",
    "\n",
    "pad_char_seq = encoded_sentence_char['padded_chr_level']\n",
    "print(len(pad_char_seq))\n",
    "\n",
    "\n",
    "pad_sen_lab , clean_ta = encoded_labels_data['pad_sen'] , encoded_labels_data['clean_tags']\n",
    "print(len(pad_sen_lab),len(clean_ta))\n",
    "\n",
    "\n",
    "\n",
    "train_data_int = int(len(padded_sentence_)*0.90)\n",
    "\n",
    "train_padded_sentence , train_sequence_lengths = encoded_sentence_word['padded_sentence'][:train_data_int] , encoded_sentence_word['sequence_len'][:train_data_int]\n",
    "train_char_sentence  = encoded_sentence_char['padded_chr_level'][:train_data_int]\n",
    "train_tag = encoded_labels_data['pad_sen'][:train_data_int]\n",
    "\n",
    "iteration_ = int(len(train_padded_sentence)/(batch))\n",
    "\n",
    "\n",
    "test_padded_sentence , test_sequence_lengths = encoded_sentence_word['padded_sentence'][train_data_int:] , encoded_sentence_word['sequence_len'][train_data_int:]\n",
    "test_char_sentence  = encoded_sentence_char['padded_chr_level'][train_data_int:]\n",
    "test_tag = encoded_labels_data['pad_sen'][train_data_int:]\n",
    "\n",
    "train_batch=100\n",
    "\n",
    "test_iteration = int(len(test_padded_sentence)//train_batch)\n",
    "\n",
    "def get_train_batch(i,batch_size):\n",
    "    train_sentenc = train_padded_sentence[i*batch_size:(i+1)*batch_size]\n",
    "    train_sequence_lengt = train_sequence_lengths[i*batch_size:(i+1)*batch_size]\n",
    "    train_char_sente   =  train_char_sentence[i*batch_size:(i+1)*batch_size]\n",
    "    train_tags         = train_tag[i*batch_size:(i+1)*batch_size]\n",
    "    \n",
    "    \n",
    "    return train_sentenc,train_sequence_lengt,train_char_sente,train_tags\n",
    "\n",
    "def get_test_batch(i,batch_size):\n",
    "    train_sentenc = test_padded_sentence[i*batch_size:(i+1)*batch_size]\n",
    "    train_sequence_lengt = test_sequence_lengths[i*batch_size:(i+1)*batch_size]\n",
    "    train_char_sente   =  test_char_sentence[i*batch_size:(i+1)*batch_size]\n",
    "    train_tags         = test_tag[i*batch_size:(i+1)*batch_size]\n",
    "    \n",
    "    return train_sentenc,train_sequence_lengt,train_char_sente,train_tags\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(word_embeda))\n",
    "# print(len(padded_sentence_),len(sequence_lengths))\n",
    "# print(len(pad_char_seq))\n",
    "# print(len(pad_sen_lab))\n",
    "# print(len(clean_ta))\n",
    "# print('before__')\n",
    "# new_= np.delete(word_embeda,[162])\n",
    "# print('sh',len(new_))\n",
    "# # del padded_sentence_[162]\n",
    "# # del sequence_lengths[162]\n",
    "# # del pad_char_seq[162]\n",
    "# # del pad_sen_lab[162]\n",
    "# # del clean_ta[162]\n",
    "# print(len(word_embeda))\n",
    "# print(len(padded_sentence_),len(sequence_lengths))\n",
    "# print(len(pad_char_seq))\n",
    "# print(len(pad_sen_lab))\n",
    "# print(len(clean_ta))\n",
    "# print('after__')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://arxiv.org/abs/1603.01354\n",
    "#@monk \\m/\n",
    "\n",
    "\n",
    "class CNN_BILSTM_CRF_NETWORK (object):\n",
    "    \n",
    "    def __init__(self,batch_size,word_length,sentence_length,word_dim,chr_vocab,char_dim,labels_categories,word_vocab,word_em=None,word_embeda=None):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        \n",
    "        #placeholders\n",
    "        input_sentences       = tf.placeholder(name='sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None])\n",
    "        input_char_sentence   = tf.placeholder(name='char_sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None,None])\n",
    "        labels                = tf.placeholder(name='labels',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None])\n",
    "        sequence_len          = tf.placeholder(name='sequence_lengths',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,])\n",
    "        keep_prob =             tf.placeholder(tf.float32, \n",
    "                                               name='keep_prob')\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.placeholders_inputs = {\n",
    "                                   'input_sentence':input_sentences,\n",
    "                                   'char_sentence':input_char_sentence,\n",
    "                                   'labels':labels,\n",
    "                                   'sequence_length':sequence_len,\n",
    "                                   'prob':keep_prob\n",
    "                                  }\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if word_em:\n",
    "            \n",
    "            # if word embedding provided\n",
    "            embed = np.array(word_embeda)\n",
    "            shape = embed.shape\n",
    "            \n",
    "            word_embedding = tf.get_variable(name=\"Word_embedding\", \n",
    "                                             shape=shape, \n",
    "                                             initializer=tf.constant_initializer(embed), \n",
    "                                             trainable=False)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            #use random\n",
    "            word_embedding = tf.get_variable(name='word_embedding',\n",
    "                                             shape=[word_vocab,word_dim],\n",
    "                                             initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                             dtype=tf.float32)\n",
    "            \n",
    "        #char embedding for cnn\n",
    "        chr_embedding = tf.get_variable(name='chr_embedding',\n",
    "                                        shape=[chr_vocab,char_dim],\n",
    "                                        initializer=tf.random_uniform_initializer(-tf.sqrt(3.0/char_dim),tf.sqrt(3.0/char_dim)),\n",
    "                                        dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #embedd input with embedding dim\n",
    "        word_embedd_input = tf.nn.embedding_lookup(word_embedding,input_sentences)\n",
    "        char_embedd_input = tf.nn.embedding_lookup(chr_embedding,input_char_sentence)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('cnn_network'):\n",
    "            # CNN for Character-level Representation\n",
    "            # A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN.\n",
    "            # CNN window size 3\n",
    "            # number of filters 30\n",
    "            cnn_filter = tf.get_variable(name='filter',\n",
    "                                     shape=[3,char_dim,1,30],\n",
    "                                     initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                     dtype=tf.float32)\n",
    "            \n",
    "            cnn_bias   = tf.get_variable(name='cnn_bias',\n",
    "                                    shape=[30],\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                    dtype=tf.float32)\n",
    "            \n",
    "            #input dim and filter dim must match\n",
    "            cnn_input = tf.reshape(char_embedd_input,[-1,word_length,char_dim,1])\n",
    "        \n",
    "            #dropout_applied\n",
    "            cnn_input = tf.nn.dropout(cnn_input,keep_prob=keep_prob)\n",
    "            cnn_network = tf.add( tf.nn.conv2d(cnn_input,cnn_filter,\n",
    "                                               strides=[1,1,1,1],\n",
    "                                               padding='VALID') , \n",
    "                                 cnn_bias )\n",
    "        \n",
    "            relu_applied = tf.nn.relu(cnn_network)\n",
    "        \n",
    "        \n",
    "            #2 words for pad and unknown\n",
    "            max_pool     = tf.nn.max_pool(relu_applied,\n",
    "                                          ksize=[1,word_length-2,1,1],\n",
    "                                          strides=[1,1,1,1],\n",
    "                                          padding='VALID')\n",
    "        \n",
    "        \n",
    "            #reshape for bilstm_network\n",
    "            cnn_output   = tf.reshape(max_pool,[-1,sentence_length,30])\n",
    "            \n",
    "            \n",
    "        word_embedding_reshape = tf.reshape(word_embedd_input,[-1,max_sentence,word_dim])\n",
    "        \n",
    "        # character-level representation vector is concatenated\n",
    "        # with the word embedding vector to feed into\n",
    "        # the BLSTM network.\n",
    "    \n",
    "        concat_operation = tf.concat([cnn_output,word_embedding_reshape],2)\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.variable_scope('bi-lstm_network'):\n",
    "            \n",
    "              #  dropout layers are applied on both the input\n",
    "              #  and output vectors of BLSTM.\n",
    "            lstm_input = tf.nn.dropout(concat_operation,keep_prob=keep_prob)\n",
    "            \n",
    "            #hidden_units = 200\n",
    "            forward_cell = tf.contrib.rnn.LSTMCell(num_units=200)\n",
    "            backward_cell = tf.contrib.rnn.LSTMCell(num_units=200)\n",
    "            \n",
    "            model_output , (fs,fw) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_cell,\n",
    "                                                                     cell_bw=backward_cell,\n",
    "                                                                     inputs=lstm_input,\n",
    "                                                                     sequence_length=sequence_len,\n",
    "                                                                     dtype=tf.float32)\n",
    "            \n",
    "            # Instead of taking last output concat and reshape\n",
    "            rnn_output_con = tf.concat([model_output[0],model_output[1]],2)\n",
    "            \n",
    "            \n",
    "            #hidden_units = 200 since its bidirectional so 2*200\n",
    "        rnn_output = tf.reshape(rnn_output_con,[-1,2*200])\n",
    "        rnn_output_drop = tf.nn.dropout(rnn_output,\n",
    "                                        keep_prob=keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #linear_projection without activation function\n",
    "        \n",
    "        projection_weight = tf.get_variable(name='projection_weight',\n",
    "                                           shape=[2*200,labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        projection_bias   = tf.get_variable(name='bias',\n",
    "                                           shape=[labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        projection        =  tf.add(tf.matmul(rnn_output_drop,projection_weight),projection_bias)\n",
    "        \n",
    "        projection_output = tf.reshape(projection,[-1,sentence_length,\n",
    "                                                   labels_categories])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # calculate_loss\n",
    "        \n",
    "        # for more explanation I've added docs for this function\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(projection_output,\n",
    "                                                                              labels,\n",
    "                                                                              sequence_len)\n",
    "        \n",
    "        loss    = -tf.reduce_mean(log_likelihood)\n",
    "        \n",
    "        \n",
    "        #training\n",
    "        \n",
    "        train  = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9)\n",
    "\n",
    "        grad_s = train.compute_gradients(loss)\n",
    "        grad_clip = [[tf.clip_by_value(grad_,-5,5) , var_] for grad_,var_ in grad_s]\n",
    "        \n",
    "        apply_gradient = train.apply_gradients(grad_clip)\n",
    "        \n",
    "        self.train_operation = {'train':apply_gradient}\n",
    "        \n",
    "        \n",
    "        self.network_output  = {'loss':loss,'logits':projection_output,'trans_matrix':transition_params}\n",
    "        \n",
    "        #caculate viterbi score and labels \n",
    "        \n",
    "        def viterbi_algorithm(sequence_length_,logits_,labels_,transition_):\n",
    "\n",
    "            predicted_labels =[]\n",
    "            real_labels = []\n",
    "            for seq_len, logits_data , label_ in zip(sequence_length_,logits_,labels_):\n",
    "                score_ = np.array(logits_data[:seq_len]).astype(np.float32)\n",
    "                \n",
    "\n",
    "                #     print(score)           #seq_len x num_tags    \n",
    "                #     print(transition_params)#num_tags x num_tags\n",
    "\n",
    "                viterbi,viterbi_score = tf.contrib.crf.viterbi_decode(score_,transition_)\n",
    "                predicted_labels.append(viterbi)\n",
    "                real_labels.append(label_[:seq_len])\n",
    "\n",
    "            return {\n",
    "\n",
    "                    'predicted_labels' : predicted_labels  , \n",
    "                    'real_labels' : real_labels \n",
    "\n",
    "                   }\n",
    "        \n",
    "        \n",
    "        self.viterbi = {'viterbi_algo' : viterbi_algorithm }\n",
    "        \n",
    "        #calculate accuracy basis on viterbi \n",
    "        \n",
    "        def accuracy_calculation(real_,predicted_):\n",
    "\n",
    "\n",
    "            score_ = []\n",
    "            \n",
    "            assert len(real_)==len(predicted_) , 'length should be same'\n",
    "\n",
    "            for combined in zip(real_,predicted_):\n",
    "                if combined[0]==combined[1]:\n",
    "                    score_.append(1)\n",
    "                else:\n",
    "                    score_.append(0)\n",
    "\n",
    "            return score_.count(1)/len(score_)\n",
    "        \n",
    "        self.accuracy_calculation = {'accuracy_calculation': accuracy_calculation}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "epoch_no = 10\n",
    "def execute_model(model):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(iteration_ ,test_iteration)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for m in range(epoch_no):\n",
    "            for j in range(iteration_):\n",
    "                \n",
    "                if j==162:\n",
    "                    pass\n",
    "                else:\n",
    "                    get_da = get_train_batch(j,10)\n",
    "                    print(np.array(get_da[0]).shape)\n",
    "                    \n",
    "                    output,_ = sess.run([model.network_output,model.train_operation],feed_dict={model.placeholders_inputs['input_sentence']:get_da[0],\n",
    "                                                              model.placeholders_inputs['char_sentence']:get_da[2],model.placeholders_inputs['labels']:get_da[3],\n",
    "                                                             model.placeholders_inputs['sequence_length']:get_da[1],\n",
    "                                                                                   model.placeholders_inputs['prob']:0.5})\n",
    "\n",
    "                    data= model.viterbi['viterbi_algo'](get_da[1],output['logits'],get_da[3],output['trans_matrix'])\n",
    "                    predicted = data['predicted_labels']\n",
    "                    real_la   = data['real_labels']\n",
    "                    \n",
    "                    if j%500==0:\n",
    "                        batch_accu = []\n",
    "                        for i in zip(predicted,real_la):\n",
    "                            batch_accu.append(model.accuracy_calculation['accuracy_calculation'](i[0],i[1]))\n",
    "                        print('loss',output['loss'],'accuracy',np.mean(batch_accu),'iteration',j,'epoch',m)\n",
    "\n",
    "\n",
    " \n",
    "                for j in range(test_iteration):\n",
    "                    get_daa = get_test_batch(j,100)\n",
    "                    output_test = sess.run(model.network_output,feed_dict={model.placeholders_inputs['input_sentence']:get_daa[0],\n",
    "                                                              model.placeholders_inputs['char_sentence']:get_daa[2],model.placeholders_inputs['labels']:get_daa[3],\n",
    "                                                             model.placeholders_inputs['sequence_length']:get_daa[1],\n",
    "                                                                                   model.placeholders_inputs['prob']:1})\n",
    "                    data_order = model.viterbi['viterbi_algo'](get_daa[1],output['logits'],get_daa[3],output['trans_matrix'])\n",
    "                    predicted_order = data_order['predicted_labels']\n",
    "                    real_la_order   = data_order['real_labels']\n",
    "\n",
    "                    batch_accu_test = []\n",
    "                    for i in zip(predicted_order,real_la_order):\n",
    "                        batch_accu_test.append(model.accuracy_calculation['accuracy_calculation'](i[0],i[1]))\n",
    "                        print('test_accuracy','loss',output['loss'],'accuracy',np.mean(batch_accu_test),j)\n",
    "                        write_file(m,np.mean(batch_accu_test))\n",
    "\n",
    "\n",
    "#                         if j%100==0:\n",
    "\n",
    "#                             datar=[1,2,3,4,5,6,7,8,9]\n",
    "#                             choose_one = random.choice(datar)\n",
    "\n",
    "\n",
    "#                             print(\"test_data_accuracy\",predicted[choose_one],'vs \\n\\n' ,real_la[choose_one])\n",
    "                        \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    model = CNN_BILSTM_CRF_NETWORK(10,35,115,100,86,30,4,21010)\n",
    "    execute_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
