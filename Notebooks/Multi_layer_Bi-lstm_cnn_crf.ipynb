{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "#https://arxiv.org/abs/1603.01354\n",
    "#@monk \\m/\n",
    "\n",
    "\n",
    "class CNN_BILSTM_CRF_NETWORK (object):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self,batch_size,\n",
    "                 word_length,\n",
    "                 sentence_length,\n",
    "                 word_dim,\n",
    "                 chr_vocab,\n",
    "                 char_dim,\n",
    "                 labels_categories,\n",
    "                 hidden_uni,\n",
    "                 lstm_layers,\n",
    "                 word_em=None,\n",
    "                 word_embeda=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #placeholders\n",
    "        self.input_sentences       = tf.placeholder(name='sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None])\n",
    "        self.input_char_sentence   = tf.placeholder(name='char_sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None,None])\n",
    "        self.labels                = tf.placeholder(name='labels',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None,None])\n",
    "        self.sequence_len          = tf.placeholder(name='sequence_lengths',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[None])\n",
    "        self.keep_prob =             tf.placeholder(tf.float32, \n",
    "                                               name='keep_prob')\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            # if word embedding provided\n",
    "        embed = np.array(word_embeda)\n",
    "        shape = embed.shape\n",
    "        \n",
    "        word_embedding = tf.get_variable(name=\"Word_embedding\", \n",
    "                                             shape=shape, \n",
    "                                             initializer=tf.constant_initializer(embed), \n",
    "                                             trainable=False)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             #use random\n",
    "#             word_embedding = tf.get_variable(name='word_embedding',\n",
    "#                                              shape=[word_vocab,word_dim],\n",
    "#                                              initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "#                                              dtype=tf.float32)\n",
    "            \n",
    "        #char embedding for cnn\n",
    "        chr_embedding = tf.get_variable(name='chr_embedding',\n",
    "                                        shape=[chr_vocab,char_dim],\n",
    "                                        initializer=tf.random_uniform_initializer(-tf.sqrt(3.0/char_dim),tf.sqrt(3.0/char_dim)),\n",
    "                                        dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #embedd input with embedding dim\n",
    "        word_embedd_input = tf.nn.embedding_lookup(word_embedding,self.input_sentences)\n",
    "        char_embedd_input = tf.nn.embedding_lookup(chr_embedding,self.input_char_sentence)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('cnn_network'):\n",
    "            # CNN for Character-level Representation\n",
    "            # A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN.\n",
    "            # CNN window size 3\n",
    "            # number of filters 30\n",
    "            cnn_filter = tf.get_variable(name='filter',\n",
    "                                     shape=[3,char_dim,1,30],\n",
    "                                     initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                     dtype=tf.float32)\n",
    "            \n",
    "            cnn_bias   = tf.get_variable(name='cnn_bias',\n",
    "                                    shape=[30],\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                    dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #input dim and filter dim must match\n",
    "            cnn_input = tf.reshape(char_embedd_input,[-1,word_length,char_dim,1])\n",
    "        \n",
    "            #dropout_applied\n",
    "            cnn_input = tf.nn.dropout(cnn_input,keep_prob=self.keep_prob)\n",
    "            cnn_network = tf.add( tf.nn.conv2d(cnn_input,cnn_filter,\n",
    "                                               strides=[1,1,1,1],\n",
    "                                               padding='VALID') , \n",
    "                                 cnn_bias )\n",
    "        \n",
    "            relu_applied = tf.nn.relu(cnn_network)\n",
    "        \n",
    "        \n",
    "            #2 words for pad and unknown\n",
    "            max_pool     = tf.nn.max_pool(relu_applied,\n",
    "                                          ksize=[1,word_length-2,1,1],\n",
    "                                          strides=[1,1,1,1],\n",
    "                                          padding='VALID')\n",
    "        \n",
    "        \n",
    "            #reshape for bilstm_network\n",
    "            cnn_output   = tf.reshape(max_pool,[-1,sentence_length,30])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        word_embedding_reshape = tf.reshape(word_embedd_input,[-1,sentence_length,word_dim])\n",
    "        \n",
    "        # character-level representation vector is concatenated\n",
    "        # with the word embedding vector to feed into\n",
    "        # the BLSTM network.\n",
    "    \n",
    "        concat_operation = tf.concat([cnn_output,word_embedding_reshape],2)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        with tf.variable_scope('encoder'):\n",
    "            # forward cell of Bi-directional lstm network\n",
    "            #  dropout layers are applied on both the input\n",
    "            #  and output vectors of BLSTM.\n",
    "            \n",
    "            lstm_input = tf.nn.dropout(concat_operation,keep_prob=self.keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "            def fr_cell():\n",
    "                fr_cell_lstm = tf.contrib.rnn.LSTMCell(num_units=hidden_uni, forget_bias=1.0)\n",
    "\n",
    "                return tf.contrib.rnn.DropoutWrapper(cell=fr_cell_lstm, output_keep_prob=self.keep_prob, dtype=tf.float32)\n",
    "                # dropout layer for forward cell\n",
    "\n",
    "            # Forward RNNCells as its inputs and wraps them into a single cell\n",
    "\n",
    "            fr_cell_m = tf.contrib.rnn.MultiRNNCell([fr_cell() for _ in range(lstm_layers)], state_is_tuple=True)\n",
    "        # fr_initial_cell = fr_cell_m.zero_state(batch_size=batch_size,dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.variable_scope('encoder'):\n",
    "            # backward cell for Bi-directional lstm network\n",
    "\n",
    "\n",
    "\n",
    "            def bw_cell():\n",
    "                bw_cell_lstm = tf.contrib.rnn.LSTMCell(num_units=hidden_uni, forget_bias=1.0)\n",
    "\n",
    "                return tf.contrib.rnn.DropoutWrapper(cell=bw_cell_lstm, output_keep_prob=self.keep_prob, dtype=tf.float32)\n",
    "                # droput layer for backward cell\n",
    "\n",
    "            # Backward RNNCells as its inputs and wraps them into a single cell\n",
    "\n",
    "            bw_cell_m = tf.contrib.rnn.MultiRNNCell([bw_cell() for _ in range(lstm_layers)], state_is_tuple=True)\n",
    "\n",
    "\n",
    "            # bw_initial_cell = bw_cell_m.zero_state(batch_size=batch_size,dtype=tf.float32)\n",
    "\n",
    "\n",
    "            # return of Bi-directional  lstm network  :\n",
    "\n",
    "            # A tuple (outputs, output_states) where:\n",
    "\n",
    "            # outputs       :        A tuple (output_fw, output_bw) containing the forward and the backward rnn output Tensor #batch_size, max_time, cell_fw.output_size]\n",
    "            # output_states :        A tuple ( state.cT , state.hT ) containing the shape [Batch_size,num_inputs] and has the final cell state cT and output state hT of each batch sequence.\n",
    "\n",
    "        # Bi-directional lstm network\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            model, (state_c, state_h) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=fr_cell_m,  # forward cell\n",
    "                cell_bw=bw_cell_m,  # backward cell\n",
    "                inputs=lstm_input,  # 3 dim embedding input for rnn\n",
    "                sequence_length=self.sequence_len,  # sequence len == batch_size\n",
    "\n",
    "                # initial_state_fw=fr_initial_cell,\n",
    "\n",
    "                # initial_state_bw=bw_initial_cell,\n",
    "                dtype=tf.float32\n",
    "\n",
    "            )\n",
    "            \n",
    "        rnn_output_con = tf.concat([model[0],model[1]],2)\n",
    "            \n",
    "            \n",
    "            #hidden_units = 200 since its bidirectional so 2*200\n",
    "        rnn_output = tf.reshape(rnn_output_con,[-1,2*hidden_uni])\n",
    "        rnn_output_drop = tf.nn.dropout(rnn_output,\n",
    "                                        keep_prob=self.keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #linear_projection without activation function\n",
    "        \n",
    "        projection_weight = tf.get_variable(name='projection_weight',\n",
    "                                           shape=[2*hidden_uni,labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        projection_bias   = tf.get_variable(name='bias',\n",
    "                                           shape=[labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        projection        =  tf.add(tf.matmul(rnn_output_drop,projection_weight),projection_bias)\n",
    "        \n",
    "        self.projection_output = tf.reshape(projection,[-1,sentence_length,\n",
    "                                                   labels_categories])\n",
    "        \n",
    "        self.probability_distribution = tf.nn.softmax(self.projection_output, name='netout')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # calculate_loss\n",
    "        \n",
    "        # for more explanation I've added docs for this function\n",
    "        self.log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(self.projection_output,\n",
    "                                                                              self.labels,\n",
    "                                                                              self.sequence_len)\n",
    "        \n",
    "        self.loss    = -tf.reduce_mean(self.log_likelihood)\n",
    "        \n",
    "        \n",
    "        #training\n",
    "        \n",
    "        train  = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9)\n",
    "\n",
    "        grad_s = train.compute_gradients(self.loss)\n",
    "        grad_clip = [[tf.clip_by_value(grad_,-5.0,5.0) , var_] for grad_,var_ in grad_s]\n",
    "        \n",
    "        apply_gradient = train.apply_gradients(grad_clip)\n",
    "        \n",
    "        self.train_operation = apply_gradient\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def viterbi_algorithm(self,sequence_length_,logits_,labels_,transition_):\n",
    "\n",
    "        predicted_labels =[]\n",
    "        real_labels = []\n",
    "        for seq_len, logits_data , label_ in zip(sequence_length_,logits_,labels_):\n",
    "            score_ = np.array(logits_data[:seq_len]).astype(np.float32)\n",
    "\n",
    "\n",
    "            #     print(score)           #seq_len x num_tags    \n",
    "            #     print(transition_params)#num_tags x num_tags\n",
    "\n",
    "            viterbi,viterbi_score = tf.contrib.crf.viterbi_decode(score_,transition_)\n",
    "            predicted_labels.append(viterbi)\n",
    "            real_labels.append(viterbi_score)\n",
    "\n",
    "        return {\n",
    "\n",
    "                'predicted_labels' : predicted_labels  , \n",
    "                'real_labels' : real_labels \n",
    "\n",
    "               }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def accuracy_calculation(self,real_,predicted_,seq_len_p):\n",
    "        score_ = []\n",
    "        assert len(real_)==len(predicted_) , 'length should be same'\n",
    "        \n",
    "        for real_h,seq_lenh,predicted_h in zip(real_,predicted_,seq_len_p):\n",
    "            real_data = real_h[:predicted_h]\n",
    "            predicted_data = seq_lenh\n",
    "            cal=[]\n",
    "            for h,j in zip(real_data,predicted_data):\n",
    "                if h==j and h!=3 and j!=3:\n",
    "                    cal.append(1)\n",
    "                elif h!=j and h!=3:\n",
    "                    cal.append(0)\n",
    "                \n",
    "            score_.append(cal.count(1)/len(cal))\n",
    "        \n",
    "        return cal.count(1)/len(cal)\n",
    "            \n",
    "    \n",
    "            \n",
    "        \n",
    "#         for combined in zip(real_,predicted_):\n",
    "#             if combined[0]==combined[1]:\n",
    "#                     score_.append(1)\n",
    "#             else:\n",
    "#                     score_.append(0)\n",
    "\n",
    "#         return score_.count(1)/len(score_)\n",
    "        \n",
    "#         self.accuracy_calculation = {'accuracy_calculation': accuracy_calculation}\n",
    "        \n",
    "    def train(self,sess):\n",
    "        \n",
    "\n",
    "        \n",
    "        if os.path.exists('Modelsmulti/model.ckpt.meta'): \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, './Modelsmulti/model.ckpt')\n",
    "        else:\n",
    "            tf.global_variables_initializer().run()\n",
    "        num_batch = len(train_words2ids) // 10\n",
    "        for epo in range(1):\n",
    "            for i in range(50):\n",
    "                batch_words_ids = np.array(train_words2ids[i*10 : (i+1)*10])\n",
    "                batch_chars_ids = np.array(train_chars2ids[i*10 : (i+1)*10]) \n",
    "                batch_labels = np.array(train_labels2ids[i*10 : (i+1)*10])\n",
    "                batch_sequence_lengths = np.array(train_sequence_lengths[i*10 : (i+1)*10])\n",
    "                \n",
    "\n",
    "                \n",
    "                _, predication, show_loss, trans_matrix = sess.run([self.train_operation, self.projection_output, self.loss, self.transition_params], \n",
    "                                                     feed_dict = {self.input_sentences: batch_words_ids, \n",
    "                                                                  self.input_char_sentence: batch_chars_ids,\n",
    "                                                                  self.labels: batch_labels,\n",
    "                                                                  self.sequence_len: batch_sequence_lengths,self.keep_prob:0.5})\n",
    "                data_d = self.viterbi_algorithm(batch_sequence_lengths,predication,batch_labels,trans_matrix)\n",
    "                \n",
    "#                 p = self.evaluate(\n",
    "#                         10, data_d['predicted_labels'], batch_labels, batch_sequence_lengths)\n",
    "\n",
    "                p=self.accuracy_calculation(batch_labels,data_d['predicted_labels'],batch_sequence_lengths)\n",
    "\n",
    "    \n",
    "                if i % 10 == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d], loss: %.8f, accurate = %.4f\"% (epo, i, num_batch, show_loss, p))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'Modelsmulti/model.ckpt')\n",
    "        \n",
    "    def test(self, sess, ids2labels, label_num):        \n",
    "        predict_label = []\n",
    "        test_label = []         \n",
    "        batch_num = len(test_words2ids) // 10\n",
    "        for i in range(batch_num):\n",
    "            batch_words_ids = np.array(test_words2ids[i*10 : (i+1)*10])\n",
    "            batch_chars_ids = np.array(test_chars2ids[i*10 : (i+1)*10]) \n",
    "            batch_labels = np.array(test_labels2ids[i*10 : (i+1)*10])\n",
    "            batch_sequence_lengths = np.array(test_sequence_lengths[i*10 : (i+1)*10])\n",
    "            \n",
    "            predication, trans_matrix = sess.run([self.projection_output, self.transition_params], \n",
    "                                                 feed_dict = {self.input_sentences: batch_words_ids, \n",
    "                                                                  self.input_char_sentence: batch_chars_ids,\n",
    "                                                                  self.labels: batch_labels,\n",
    "                                                                  self.sequence_len: batch_sequence_lengths,self.keep_prob:1})\n",
    "            predicts = self.viterbi_algorithm(batch_sequence_lengths,predication,batch_labels,trans_matrix)\n",
    "            \n",
    "            \n",
    "            \n",
    "            for j in range(10):\n",
    "                sentence_len = batch_sequence_lengths[j]\n",
    "                test_label.extend(batch_labels[j][:sentence_len])\n",
    "                \n",
    "                \n",
    "                \n",
    "                predict_label.extend(predicts['predicted_labels'][j])\n",
    "            if i % 50 == 0:\n",
    "                print('process completed %d %%, please be patient' %int(i/batch_num*100))\n",
    "        target_names = []\n",
    "        for i, label in enumerate(test_label):\n",
    "            test_label[i] = label - 1 \n",
    "        for i, label in enumerate(predict_label):\n",
    "            predict_label[i] = label - 1         \n",
    "        for i in range(label_num):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            target_names.append(ids2labels[i])\n",
    "\n",
    "        print(metrics.classification_report(test_label, predict_label, target_names=target_names))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
