{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "with open('word_embedding','rb') as f: \n",
    "    word_embedding = pk.load(f)\n",
    "        \n",
    "with open('char_embedding','rb') as f: \n",
    "    char_embedding = pk.load(f)\n",
    "        \n",
    "with open('train_words2ids','rb') as f: \n",
    "    train_words2ids = pk.load(f)\n",
    "        \n",
    "        \n",
    "with open('train_chars2ids','rb') as f: \n",
    "    train_chars2ids = pk.load(f)\n",
    "        \n",
    "with open('train_labels2ids','rb') as f: \n",
    "    train_labels2ids = pk.load(f)\n",
    "        \n",
    "        \n",
    "with open('train_sequence_lengths','rb') as f: \n",
    "    train_sequence_lengths = pk.load(f)\n",
    "        \n",
    "with open('test_words2ids','rb') as f: \n",
    "    test_words2ids = pk.load(f)\n",
    "        \n",
    "with open('test_chars2ids','rb') as f: \n",
    "    test_chars2ids = pk.load(f)\n",
    "        \n",
    "with open('test_labels2ids','rb') as f: \n",
    "    test_labels2ids = pk.load(f)\n",
    "        \n",
    "with open('test_sequence_lengths','rb') as f: \n",
    "    test_sequence_lengths = pk.load(f)\n",
    "        \n",
    "with open('label_num','rb') as f: \n",
    "    label_num = pk.load(f)\n",
    "    \n",
    "with open('ids2labels','rb') as f: \n",
    "    ids2labels = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://arxiv.org/abs/1603.01354#https: \n",
    "#@monk \\m/\n",
    "\n",
    "\n",
    "class CNN_BILSTM_CRF_NETWORK (object):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self,batch_size,word_length,sentence_length,word_dim,chr_vocab,char_dim,labels_categories,word_em=None,word_embeda=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #placeholders\n",
    "        self.input_sentences       = tf.placeholder(name='sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[batch_size,sentence_length])\n",
    "        self.input_char_sentence   = tf.placeholder(name='char_sentence',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[batch_size,sentence_length,word_length])\n",
    "        self.labels                = tf.placeholder(name='labels',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[batch_size,sentence_length])\n",
    "        self.sequence_len          = tf.placeholder(name='sequence_lengths',\n",
    "                                               dtype=tf.int32,\n",
    "                                               shape=[batch_size])\n",
    "        self.keep_prob =             tf.placeholder(tf.float32, \n",
    "                                               name='keep_prob')\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            # if word embedding provided\n",
    "        embed = np.array(word_embeda)\n",
    "        shape = embed.shape\n",
    "        \n",
    "        word_embedding = tf.get_variable(name=\"Word_embedding\", \n",
    "                                             shape=shape, \n",
    "                                             initializer=tf.constant_initializer(embed), \n",
    "                                             trainable=False)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             #use random\n",
    "#             word_embedding = tf.get_variable(name='word_embedding',\n",
    "#                                              shape=[word_vocab,word_dim],\n",
    "#                                              initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "#                                              dtype=tf.float32)\n",
    "            \n",
    "        #char embedding for cnn\n",
    "        chr_embedding = tf.get_variable(name='chr_embedding',\n",
    "                                        shape=[chr_vocab,char_dim],\n",
    "                                        initializer=tf.random_uniform_initializer(-tf.sqrt(3.0/char_dim),tf.sqrt(3.0/char_dim)),\n",
    "                                        dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #embedd input with embedding dim\n",
    "        word_embedd_input = tf.nn.embedding_lookup(word_embedding,self.input_sentences)\n",
    "        char_embedd_input = tf.nn.embedding_lookup(chr_embedding,self.input_char_sentence)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('cnn_network'):\n",
    "            # CNN for Character-level Representation\n",
    "            # A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN.\n",
    "            # CNN window size 3\n",
    "            # number of filters 30\n",
    "            cnn_filter = tf.get_variable(name='filter',\n",
    "                                     shape=[3,char_dim,1,30],\n",
    "                                     initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                     dtype=tf.float32)\n",
    "            \n",
    "            cnn_bias   = tf.get_variable(name='cnn_bias',\n",
    "                                    shape=[30],\n",
    "                                    initializer=tf.random_uniform_initializer(-0.01,0.01),\n",
    "                                    dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #input dim and filter dim must match\n",
    "            cnn_input = tf.reshape(char_embedd_input,[-1,word_length,char_dim,1])\n",
    "        \n",
    "            #dropout_applied\n",
    "            cnn_input = tf.nn.dropout(cnn_input,keep_prob=self.keep_prob)\n",
    "            cnn_network = tf.add( tf.nn.conv2d(cnn_input,cnn_filter,\n",
    "                                               strides=[1,1,1,1],\n",
    "                                               padding='VALID') , \n",
    "                                 cnn_bias )\n",
    "        \n",
    "            relu_applied = tf.nn.relu(cnn_network)\n",
    "        \n",
    "        \n",
    "            #2 words for pad and unknown\n",
    "            max_pool     = tf.nn.max_pool(relu_applied,\n",
    "                                          ksize=[1,word_length-2,1,1],\n",
    "                                          strides=[1,1,1,1],\n",
    "                                          padding='VALID')\n",
    "        \n",
    "        \n",
    "            #reshape for bilstm_network\n",
    "            cnn_output   = tf.reshape(max_pool,[-1,sentence_length,30])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        word_embedding_reshape = tf.reshape(word_embedd_input,[-1,sentence_length,word_dim])\n",
    "        \n",
    "        # character-level representation vector is concatenated\n",
    "        # with the word embedding vector to feed into\n",
    "        # the BLSTM network.\n",
    "    \n",
    "        concat_operation = tf.concat([cnn_output,word_embedding_reshape],2)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        with tf.variable_scope('bi-lstm_network'):\n",
    "            \n",
    "              #  dropout layers are applied on both the input\n",
    "              #  and output vectors of BLSTM.\n",
    "            lstm_input = tf.nn.dropout(concat_operation,keep_prob=self.keep_prob)\n",
    "            \n",
    "            #hidden_units = 200\n",
    "            forward_cell = tf.contrib.rnn.LSTMCell(num_units=200)\n",
    "            backward_cell = tf.contrib.rnn.LSTMCell(num_units=200)\n",
    "            \n",
    "            model_output , (fs,fw) = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_cell,\n",
    "                                                                     cell_bw=backward_cell,\n",
    "                                                                     inputs=lstm_input,\n",
    "                                                                     sequence_length=self.sequence_len,\n",
    "                                                                     dtype=tf.float32)\n",
    "            \n",
    "            # Instead of taking last output concat and reshape\n",
    "            rnn_output_con = tf.concat([model_output[0],model_output[1]],2)\n",
    "            \n",
    "            \n",
    "            #hidden_units = 200 since its bidirectional so 2*200\n",
    "        rnn_output = tf.reshape(rnn_output_con,[-1,2*200])\n",
    "        rnn_output_drop = tf.nn.dropout(rnn_output,\n",
    "                                        keep_prob=self.keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #linear_projection without activation function\n",
    "        \n",
    "        projection_weight = tf.get_variable(name='projection_weight',\n",
    "                                           shape=[2*200,labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        projection_bias   = tf.get_variable(name='bias',\n",
    "                                           shape=[labels_categories],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        projection        =  tf.add(tf.matmul(rnn_output_drop,projection_weight),projection_bias)\n",
    "        \n",
    "        self.projection_output = tf.reshape(projection,[-1,sentence_length,\n",
    "                                                   labels_categories])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # calculate_loss\n",
    "        \n",
    "        # for more explanation I've added docs for this function\n",
    "        self.log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(self.projection_output,\n",
    "                                                                              self.labels,\n",
    "                                                                              self.sequence_len)\n",
    "        \n",
    "        self.loss    = -tf.reduce_mean(self.log_likelihood)\n",
    "        \n",
    "        \n",
    "        #training\n",
    "        \n",
    "        train  = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9)\n",
    "\n",
    "        grad_s = train.compute_gradients(self.loss)\n",
    "        grad_clip = [[tf.clip_by_value(grad_,-5,5) , var_] for grad_,var_ in grad_s]\n",
    "        \n",
    "        apply_gradient = train.apply_gradients(grad_clip)\n",
    "        \n",
    "        self.train_operation = apply_gradient\n",
    "        \n",
    "        \n",
    "#         self.network_output  = {'loss':loss,'logits':projection_output,'trans_matrix':transition_params}\n",
    "        \n",
    "        #caculate viterbi score and labels \n",
    "        \n",
    "    \n",
    "    def viterbi_algorithm(self,sequence_length_,logits_,labels_,transition_):\n",
    "\n",
    "        predicted_labels =[]\n",
    "        real_labels = []\n",
    "        for seq_len, logits_data , label_ in zip(sequence_length_,logits_,labels_):\n",
    "            score_ = np.array(logits_data[:seq_len]).astype(np.float32)\n",
    "\n",
    "\n",
    "            #     print(score)           #seq_len x num_tags    \n",
    "            #     print(transition_params)#num_tags x num_tags\n",
    "\n",
    "            viterbi,viterbi_score = tf.contrib.crf.viterbi_decode(score_,transition_)\n",
    "            predicted_labels.append(viterbi)\n",
    "            real_labels.append(viterbi_score)\n",
    "\n",
    "        return {\n",
    "\n",
    "                'predicted_labels' : predicted_labels  , \n",
    "                'real_labels' : real_labels \n",
    "\n",
    "               }\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    def accuracy_calculation(self,real_,predicted_,seq_len_p):\n",
    "        score_ = []\n",
    "        assert len(real_)==len(predicted_) , 'length should be same'\n",
    "        \n",
    "        for real_h,seq_lenh,predicted_h in zip(real_,predicted_,seq_len_p):\n",
    "            real_data = real_h[:predicted_h]\n",
    "            predicted_data = seq_lenh\n",
    "            \n",
    "            if list(real_data) == list(predicted_data):\n",
    "                score_.append(1)\n",
    "            else:\n",
    "                score_.append(0)\n",
    "                \n",
    "        return score_.count(1)/len(score_)\n",
    "    \n",
    "            \n",
    "\n",
    "        \n",
    "    def train(self,sess):\n",
    "        \n",
    "\n",
    "        \n",
    "        if os.path.exists('Model/model.ckpt.meta'): \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, './Model/model.ckpt')\n",
    "        else:\n",
    "            tf.global_variables_initializer().run()\n",
    "        num_batch = len(train_words2ids) // 10\n",
    "        for epo in range(10):\n",
    "            for i in range(num_batch):\n",
    "                batch_words_ids = np.array(train_words2ids[i*10 : (i+1)*10])\n",
    "                batch_chars_ids = np.array(train_chars2ids[i*10 : (i+1)*10]) \n",
    "                batch_labels = np.array(train_labels2ids[i*10 : (i+1)*10])\n",
    "                batch_sequence_lengths = np.array(train_sequence_lengths[i*10 : (i+1)*10])\n",
    "                \n",
    "\n",
    "                \n",
    "                _, predication, show_loss, trans_matrix = sess.run([self.train_operation, self.projection_output, self.loss, self.transition_params], \n",
    "                                                     feed_dict = {self.input_sentences: batch_words_ids, \n",
    "                                                                  self.input_char_sentence: batch_chars_ids,\n",
    "                                                                  self.labels: batch_labels,\n",
    "                                                                  self.sequence_len: batch_sequence_lengths,self.keep_prob:0.5})\n",
    "                data_d= self.viterbi_algorithm(batch_sequence_lengths,predication,batch_labels,trans_matrix)\n",
    "                p=self.accuracy_calculation(batch_labels,data_d['predicted_labels'],batch_sequence_lengths)\n",
    "\n",
    "    \n",
    "                if i % 10 == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d], loss: %.8f, accurate = %.4f\"% (epo, i, num_batch, show_loss, p))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'Model/model.ckpt')\n",
    "        \n",
    "    def test(self, sess, ids2labels, label_num):        \n",
    "        predict_label = []\n",
    "        test_label = []         \n",
    "        batch_num = len(test_words2ids) // 10\n",
    "        for i in range(batch_num):\n",
    "            batch_words_ids = np.array(test_words2ids[i*10 : (i+1)*10])\n",
    "            batch_chars_ids = np.array(test_chars2ids[i*10 : (i+1)*10]) \n",
    "            batch_labels = np.array(test_labels2ids[i*10 : (i+1)*10])\n",
    "            batch_sequence_lengths = np.array(test_sequence_lengths[i*10 : (i+1)*10])\n",
    "            \n",
    "            predication, trans_matrix = sess.run([self.projection_output, self.transition_params], \n",
    "                                                 feed_dict = {self.input_sentences: batch_words_ids, \n",
    "                                                                  self.input_char_sentence: batch_chars_ids,\n",
    "                                                                  self.labels: batch_labels,\n",
    "                                                                  self.sequence_len: batch_sequence_lengths,self.keep_prob:1})\n",
    "            predicts = self.viterbi_algorithm(batch_sequence_lengths,predication,batch_labels,trans_matrix)\n",
    "            \n",
    "            \n",
    "            \n",
    "            for j in range(10):\n",
    "                sentence_len = batch_sequence_lengths[j]\n",
    "                test_label.extend(batch_labels[j][:sentence_len])\n",
    "                \n",
    "                \n",
    "                \n",
    "                predict_label.extend(predicts['predicted_labels'][j])\n",
    "            if i % 50 == 0:\n",
    "                print('process completed %d %%, please be patient' %int(i/batch_num*100))\n",
    "        target_names = []\n",
    "        for i, label in enumerate(test_label):\n",
    "            test_label[i] = label - 1 \n",
    "        for i, label in enumerate(predict_label):\n",
    "            predict_label[i] = label - 1         \n",
    "        for i in range(label_num):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            target_names.append(ids2labels[i])\n",
    "\n",
    "        print(metrics.classification_report(test_label, predict_label, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "# flags = tf.app.flags\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "\n",
    "    print('finish converting')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print('begin training')\n",
    "        \n",
    "        model = CNN_BILSTM_CRF_NETWORK(10,70,150,100,86,30,10,word_em=1,word_embeda=word_embedding)\n",
    "        \n",
    "        \n",
    "        model.train(sess)\n",
    " \n",
    "#         model.train(sess)\n",
    "#         print('finish training')\n",
    "#         print('start testing')\n",
    "        if os.path.exists('Model/model.ckpt.meta'): \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, './Model/model.ckpt')\n",
    "\n",
    "            model.test(sess, ids2labels, label_num)\n",
    "        else:\n",
    "            raise Exception(\"[!] Train a model first\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
